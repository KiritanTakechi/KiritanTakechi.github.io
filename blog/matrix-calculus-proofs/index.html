<!doctype html><html lang=zh><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1.0" name=viewport><title>矩阵求导公式推导 | 雪間</title><meta content="本文从最基本的导数定义出发，详细推导和证明了矩阵求导（Matrix Calculus）中的核心公式。" name=description><meta content=#E9546B name=theme-color><link href=https://kiritantakechi.github.io/favicon/site.webmanifest rel=manifest><link href=https://kiritantakechi.github.io/favicon/apple-touch-icon.png rel=apple-touch-icon><meta content=雪間 name=apple-mobile-web-app-title><link href=https://kiritantakechi.github.io/favicon/favicon.svg rel=icon type=image/svg+xml><link rel="shortcut icon" href=https://kiritantakechi.github.io/favicon/favicon.ico><link href=https://fonts.googleapis.com rel=preconnect><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css2?family=Noto+Serif:ital,wght@0,400..900;1,400..900&display=swap" rel=stylesheet><link href=https://kiritantakechi.github.io/style.css rel=stylesheet><script defer src=https://kiritantakechi.github.io/js/remap-headings.js></script><link href=https://kiritantakechi.github.io/temml/Temml-Local.css rel=stylesheet><script defer src=https://kiritantakechi.github.io/temml/temml.min.js></script><script>document.addEventListener("DOMContentLoaded", function() {
        temml.renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},  // 用于块级公式
            {left: "$", right: "$", display: false}    // 用于行内公式
          ]
        });
      });</script><body><a class=skip-to-content href=#main-content>跳至内容</a><header class=site-header><div class=site-title><a href=/>雪間</a></div><nav class=main-nav><a href=/>Home</a><a href=https://kiritantakechi.github.io/about/>About</a><a href=https://kiritantakechi.github.io/blog/>Blog</a></nav></header><main id=main-content><article class=post><header class=post-header><h1 class=post-title>矩阵求导公式推导</h1><div class=post-meta><span>发布于 2025-06-27</span><span> • </span><span>预计阅读 10 分钟</span><div class=post-tags><a href=https://kiritantakechi.github.io/tags/shu-xue/>#数学</a><a href=https://kiritantakechi.github.io/tags/xian-xing-dai-shu/>#线性代数</a><a href=https://kiritantakechi.github.io/tags/ju-zhen-qiu-dao/>#矩阵求导</a><a href=https://kiritantakechi.github.io/tags/wei-ji-fen/>#微积分</a></div></div></header><nav class=post-toc><h3 class=toc-title>目录</h3><ul><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#1-he-xin-ding-yi>1. 核心定义</a><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#2-biao-liang-dui-xiang-liang-qiu-dao-scalar-by-vector>2. 标量对向量求导 (Scalar-by-Vector)</a> <ul><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#1-xian-xing-xing-shi-nabla-x-a-t-x-a>1) 线性形式：$\nabla_x (a^T x) = a$</a><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#2-ji-chu-er-ci-xing-nabla-x-x-t-x-2x>2) 基础二次型：$\nabla_x (x^T x) = 2x$</a><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#3-tong-yong-er-ci-xing-nabla-x-x-t-a-x-a-a-t-x>3) 通用二次型：$\nabla_x (x^T A x) = (A + A^T)x$</a></ul><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#3-biao-liang-dui-ju-zhen-qiu-dao-scalar-by-matrix>3. 标量对矩阵求导 (Scalar-by-Matrix)</a> <ul><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#1-ji-han-shu-xing-shi-frac-partial-partial-x-text-tr-ax-a-t>1) 迹函数形式：$\frac{\partial}{\partial X} \text{tr}(AX) = A^T$</a><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#2-xing-lie-shi-xing-shi-frac-partial-partial-x-det-x-det-x-x-1-t>2) 行列式形式：$\frac{\partial}{\partial X} \det(X) = \det(X)(X^{-1})^T$</a><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#3-dui-shu-xing-lie-shi-xing-shi-frac-partial-partial-x-log-det-x-x-1-t>3) 对数行列式形式：$\frac{\partial}{\partial X} \log(\det(X)) = (X^{-1})^T$</a></ul><li><a href=https://kiritantakechi.github.io/blog/matrix-calculus-proofs/#4-jie-lun>4. 结论</a></ul></nav><div class=post-content><p>本文旨在从最基础的导数定义出发，一步步证明那些最常用、最重要的矩阵求导公式。<h1 id=1-he-xin-ding-yi>1. 核心定义</h1><p>所有证明都将回归到导数的基本定义。<ol><li><strong>标量对向量的导数（梯度）</strong>：对于一个标量函数 $f(x)$ 和一个列向量 $x \in \mathbb{R}^n$，其梯度 $\nabla_x f(x)$ 是一个列向量，其第 $k$ 个元素是 $f$ 对 $x$ 第 $k$ 个分量 $x_k$ 的偏导数。 <div class=math-block>$$ \nabla_x f(x) = \frac{\partial f}{\partial x} = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{pmatrix} $$</div><li><strong>标量对矩阵的导数</strong>：对于一个标量函数 $f(X)$ 和一个 $m \times n$ 的矩阵 $X$，其导数是一个与 $X$ 形状相同的矩阵，其 $(k, l)$ 位置的元素是 $f$ 对 $X_{kl}$ 的偏导数。 <div class=math-block>$$ \left( \frac{\partial f}{\partial X} \right)_{kl} = \frac{\partial f}{\partial X_{kl}} $$</div> 我们将始终围绕这两个定义展开推导。</ol><h1 id=2-biao-liang-dui-xiang-liang-qiu-dao-scalar-by-vector>2. 标量对向量求导 (Scalar-by-Vector)</h1><h2 id=1-xian-xing-xing-shi-nabla-x-a-t-x-a>1) 线性形式：$\nabla_x (a^T x) = a$</h2><p><strong>证明</strong>：<p>设函数 $f(x) = a^T x$，其中 $a$ 和 $x$ 都是 $n \times 1$ 的列向量。我们将 $f(x)$ 展开为求和形式：<div class=math-block>$$ f(x) = a^T x = \sum_{i=1}^{n} a_i x_i = a_1 x_1 + a_2 x_2 + \dots + a_n x_n $$</div> 计算 $f(x)$ 对 $x$ 的任意分量 $x_k$ 的偏导数： <div class=math-block>$$ \frac{\partial f}{\partial x_k} = \frac{\partial}{\partial x_k} \left( \sum_{i=1}^{n} a_i x_i \right) = a_k $$</div> 这意味着梯度的第 $k$ 个分量就是 $a_k$。我们将所有分量组合回梯度向量中： <div class=math-block>$$ \nabla_x f(x) = \begin{pmatrix} a_1 \\ a_2 \\ \vdots \\ a_n \end{pmatrix} = a $$</div><strong>证毕。</strong><h2 id=2-ji-chu-er-ci-xing-nabla-x-x-t-x-2x>2) 基础二次型：$\nabla_x (x^T x) = 2x$</h2><p><strong>证明</strong>：<p>设函数 $f(x) = x^T x$。我们可以将其展开为分量的平方和：<div class=math-block>$$ f(x) = x^T x = \sum_{i=1}^{n} x_i^2 = x_1^2 + x_2^2 + \dots + x_n^2 $$</div> 计算 $f(x)$ 对 $x$ 的任意分量 $x_k$ 的偏导数： <div class=math-block>$$ \frac{\partial f}{\partial x_k} = \frac{\partial}{\partial x_k} \left( \sum_{i=1}^{n} x_i^2 \right) = \frac{\partial}{\partial x_k} (x_k^2) = 2x_k $$</div> 梯度的第 $k$ 个分量是 $2x_k$。我们将所有分量组合回梯度向量： <div class=math-block>$$ \nabla_x f(x) = \begin{pmatrix} 2x_1 \\ 2x_2 \\ \vdots \\ 2x_n \end{pmatrix} = 2x $$</div><strong>证毕。</strong><h2 id=3-tong-yong-er-ci-xing-nabla-x-x-t-a-x-a-a-t-x>3) 通用二次型：$\nabla_x (x^T A x) = (A + A^T)x$</h2><p><strong>证明</strong>：<p>设函数 $f(x) = x^T A x$，其中 $A$ 是一个 $n \times n$ 的矩阵。我们可以将其展开为二次型求和形式：<div class=math-block>$$ f(x) = x^T A x = \sum_{i=1}^{n} \sum_{j=1}^{n} A_{ij} x_i x_j $$</div> 我们计算 $f(x)$ 对 $x$ 的任意分量 $x_k$ 的偏导数。为了清晰地分析，我们将双重求和中与 $x_k$ 相关的项（即 $i=k$ 或 $j=k$ 的项）分离出来。 <p>这个和式可以被分解为四部分：<ol><li>$i=k$ 且 $j=k$ 的项：$A_{kk}x_k^2$<li>$i=k$ 但 $j \neq k$ 的项：$\sum_{j \neq k} A_{kj} x_k x_j$<li>$j=k$ 但 $i \neq k$ 的项：$\sum_{i \neq k} A_{ik} x_i x_k$<li>$i \neq k$ 且 $j \neq k$ 的项：$\sum_{i \neq k} \sum_{j \neq k} A_{ij} x_i x_j$</ol><p>现在我们对 $f(x)$ 关于 $x_k$ 求偏导。只有前三部分包含 $x_k$，第四部分导数为零。<div class=math-block>$$ \frac{\partial f}{\partial x_k} = \frac{\partial}{\partial x_k} \left( A_{kk}x_k^2 + \sum_{j \neq k} A_{kj} x_k x_j + \sum_{i \neq k} A_{ik} x_i x_k \right) $$</div> 分别对三项求导： <ul><li>$\frac{\partial}{\partial x_k}(A_{kk}x_k^2) = 2A_{kk}x_k$<li>$\frac{\partial}{\partial x_k}(\sum_{j \neq k} A_{kj} x_k x_j) = \sum_{j \neq k} A_{kj} x_j$<li>$\frac{\partial}{\partial x_k}(\sum_{i \neq k} A_{ik} x_i x_k) = \sum_{i \neq k} A_{ik} x_i$</ul><p>将它们加起来：<div class=math-block>$$ \frac{\partial f}{\partial x_k} = 2A_{kk}x_k + \sum_{j \neq k} A_{kj} x_j + \sum_{i \neq k} A_{ik} x_i $$</div> 现在我们进行一个巧妙的重组，把 $2A_{kk}x_k$ 分成两部分： <div class=math-block>$$ \frac{\partial f}{\partial x_k} = \left( A_{kk}x_k + \sum_{j \neq k} A_{kj} x_j \right) + \left( A_{kk}x_k + \sum_{i \neq k} A_{ik} x_i \right) $$</div> 观察括号内的两项： <ul><li>第一项是：$\sum_{j = 1}^{n} A_{kj} x_j$。这正是矩阵向量乘积 $Ax$ 的第 $k$ 个元素，记为 $(Ax)_k$。<li>第二项是：$\sum_{i = 1}^{n} A_{ik} x_i$。这可以被看作是 $A$ 的第 $k$ 列与 $x$ 的点积，也就是 $A^T$ 的第 $k$ 行与 $x$ 的点积。这正是矩阵向量乘积 $A^Tx$ 的第 $k$ 个元素，记为 $(A^Tx)_k$。</ul><p>所以，梯度的第 $k$ 个分量是：<div class=math-block>$$ \frac{\partial f}{\partial x_k} = (Ax)_k + (A^Tx)_k = ((A + A^T)x)_k $$</div> 将所有分量组合回梯度向量，我们得到： <div class=math-block>$$ \nabla_x f(x) = (A + A^T)x $$</div><strong>证毕。</strong><h1 id=3-biao-liang-dui-ju-zhen-qiu-dao-scalar-by-matrix>3. 标量对矩阵求导 (Scalar-by-Matrix)</h1><p>在掌握了对向量的求导后，我们自然地将这一思想扩展到矩阵。<h3 id=1-ji-han-shu-xing-shi-frac-partial-partial-x-text-tr-ax-a-t>1) 迹函数形式：$\frac{\partial}{\partial X} \text{tr}(AX) = A^T$</h3><p><strong>证明</strong>：<p>设函数 $f(X) = \text{tr}(AX)$。其中 $A$ 是 $m \times n$ 矩阵，$X$ 是 $n \times m$ 矩阵。其乘积 $AX$ 是 $m \times m$ 方阵。我们将迹的表达式用求和形式写出：<div class=math-block>$$ f(X) = \text{tr}(AX) = \sum_{i=1}^{m} (AX)_{ii} = \sum_{i=1}^{m} \sum_{j=1}^{n} A_{ij} X_{ji} $$</div> 我们计算 $f(X)$ 对 $X$ 的任意一个元素 $X_{kl}$ 的偏导数： <div class=math-block>$$ \frac{\partial f}{\partial X_{kl}} = \frac{\partial}{\partial X_{kl}} \left( \sum_{i=1}^{m} \sum_{j=1}^{n} A_{ij} X_{ji} \right) $$</div> 在所有项中，只有当 $X$ 的索引 $(j, i)$ 等于 $(k, l)$ 时，导数才不为零。因此，只有 $A_{lk} X_{kl}$ 这一项对求导有贡献。 <div class=math-block>$$ \frac{\partial f}{\partial X_{kl}} = A_{lk} $$</div> 这意味着导数矩阵在 $(k, l)$ 位置的元素是 $A_{lk}$。根据矩阵转置的定义，我们得到： <div class=math-block>$$ \frac{\partial f}{\partial X} = A^T $$</div><strong>证毕。</strong><h2 id=2-xing-lie-shi-xing-shi-frac-partial-partial-x-det-x-det-x-x-1-t>2) 行列式形式：$\frac{\partial}{\partial X} \det(X) = \det(X)(X^{-1})^T$</h2><p><strong>证明</strong>：<p>这个证明需要用到行列式的<strong>拉普拉斯展开</strong>。设 $f(X) = \det(X)$，其中 $X$ 是 $n \times n$ 方阵。我们将行列式沿第 $k$ 行展开：<div class=math-block>$$ f(X) = \det(X) = \sum_{j=1}^{n} X_{kj} C_{kj} $$</div> 其中 $C_{kj}$ 是元素 $X_{kj}$ 的代数余子式，其值不依赖于 $X_{kj}$。我们对 $X$ 的任意元素 $X_{kl}$ 求偏导： <div class=math-block>$$ \frac{\partial f}{\partial X_{kl}} = \frac{\partial}{\partial X_{kl}} \left( \sum_{j=1}^{n} X_{kj} C_{kj} \right) = C_{kl} $$</div> 因此，导数矩阵就是 $X$ 的代数余子式矩阵 $C$。 <div class=math-block>$$ \frac{\partial f}{\partial X} = C $$</div> 根据伴随矩阵求逆的公式 $X^{-1} = \frac{1}{\det(X)} \text{adj}(X)$，以及伴随矩阵是代数余子式矩阵的转置 $\text{adj}(X) = C^T$，我们有： <div class=math-block>$$ C^T = \det(X) X^{-1} $$</div> 对两边取转置，得到 $C = \det(X) (X^{-1})^T$。所以： <div class=math-block>$$ \frac{\partial f}{\partial X} = \det(X) (X^{-1})^T $$</div><strong>证毕。</strong><h2 id=3-dui-shu-xing-lie-shi-xing-shi-frac-partial-partial-x-log-det-x-x-1-t>3) 对数行列式形式：$\frac{\partial}{\partial X} \log(\det(X)) = (X^{-1})^T$</h2><p><strong>证明</strong>：<p>这是一个链式法则的直接应用。设 $f(X) = \log(\det(X))$，并令 $u = \det(X)$。 根据链式法则：<div class=math-block>$$ \frac{\partial f}{\partial X} = \frac{df}{du} \cdot \frac{\partial u}{\partial X} $$</div> 我们知道 $\frac{df}{du} = \frac{1}{u} = \frac{1}{\det(X)}$，并从上一个证明中知道 $\frac{\partial u}{\partial X} = \det(X)(X^{-1})^T$。 代入得： <div class=math-block>$$ \frac{\partial f}{\partial X} = \frac{1}{\det(X)} \left( \det(X)(X^{-1})^T \right) = (X^{-1})^T $$</div><strong>证毕。</strong><h1 id=4-jie-lun>4. 结论</h1><p>通过从最基本的偏导数定义出发，我们能够严谨地推导出矩阵求导中的一系列核心公式。理解这些推导过程，不仅能帮助我们记忆公式，更重要的是建立了处理更复杂向量和矩阵函数求导问题的坚实基础。</div></article><nav class=post-navigation><div class=nav-previous></div><div class=nav-next></div></nav></main><footer class=site-footer><div class=footer-content><p class=sakura-separator><img alt=Sakura class=sakura-image height=32 src=https://kiritantakechi.github.io/favicon/favicon.svg width=32><p>© 2025 Kiritan</div></footer>